step: 1, loss: 12.170757293701172
step: 2, loss: 6.803623676300049
step: 3, loss: 2.1902430057525635
step: 4, loss: 0.19133129715919495
step: 5, loss: 0.011786065995693207
step: 6, loss: 0.0002557550324127078
step: 7, loss: 5.125788084114902e-05
step: 8, loss: 4.5084602788847405e-06
step: 9, loss: 1.015659677250369e-06
step: 10, loss: 8.024811744689941
step: 11, loss: 19.51093864440918
step: 12, loss: 17.88572883605957
step: 13, loss: 15.944555282592773
step: 14, loss: 13.222338676452637
step: 15, loss: 10.144201278686523
step: 16, loss: 7.053738594055176
step: 17, loss: 4.0690460205078125
step: 18, loss: 1.178686261177063
step: 19, loss: 0.14743737876415253
step: 20, loss: 0.035241518169641495
step: 21, loss: 0.0009001466678455472
step: 22, loss: 18.16396141052246
step: 23, loss: 25.155113220214844
step: 24, loss: 24.418973922729492
step: 25, loss: 22.021682739257812
step: 26, loss: 20.559167861938477
step: 27, loss: 17.966468811035156
step: 28, loss: 15.183998107910156
step: 29, loss: 12.595917701721191
step: 30, loss: 9.447478294372559
step: 31, loss: 5.894041538238525
step: 32, loss: 3.0980799198150635
step: 33, loss: 1.4892884492874146
step: 34, loss: 14.322297096252441
step: 35, loss: 16.337499618530273
step: 36, loss: 15.932442665100098
step: 37, loss: 14.404707908630371
step: 38, loss: 12.747086524963379
step: 39, loss: 11.176322937011719
step: 40, loss: 9.077199935913086
step: 41, loss: 7.515893936157227
step: 42, loss: 4.427097320556641
step: 43, loss: 3.090754747390747
step: 44, loss: 1.3549096584320068
step: 45, loss: 1.5071734189987183
step: 46, loss: 12.664700508117676
step: 47, loss: 11.599980354309082
step: 48, loss: 10.941938400268555
step: 49, loss: 9.767426490783691
step: 50, loss: 8.04873275756836
step: 51, loss: 6.819521427154541
step: 52, loss: 4.204137802124023
step: 53, loss: 2.31264328956604
step: 54, loss: 1.004936933517456
step: 55, loss: 0.5004721283912659
step: 56, loss: 0.12684237957000732
step: 57, loss: 5.016657829284668
step: 58, loss: 8.921293258666992
step: 59, loss: 8.833995819091797
step: 60, loss: 7.96046257019043
step: 61, loss: 6.7886199951171875
step: 62, loss: 5.738790988922119
step: 63, loss: 4.073697566986084
step: 64, loss: 2.5708742141723633
step: 65, loss: 1.5146206617355347
step: 66, loss: 0.919841468334198
step: 67, loss: 0.3957618772983551
step: 68, loss: 0.1035611554980278
step: 69, loss: 7.558464527130127
step: 70, loss: 10.522178649902344
step: 71, loss: 10.144054412841797
step: 72, loss: 9.544628143310547
step: 73, loss: 8.109299659729004
step: 74, loss: 7.009979248046875
step: 75, loss: 6.1319732666015625
step: 76, loss: 5.131992340087891
step: 77, loss: 3.538320541381836
step: 78, loss: 2.677791118621826
step: 79, loss: 1.4652334451675415
step: 80, loss: 0.8562202453613281
step: 81, loss: 9.639654159545898
step: 82, loss: 9.456857681274414
step: 83, loss: 8.07725715637207
step: 84, loss: 7.691020488739014
step: 85, loss: 7.082840442657471
step: 86, loss: 5.630340099334717
step: 87, loss: 4.944886684417725
step: 88, loss: 3.8106985092163086
step: 89, loss: 2.9560837745666504
step: 90, loss: 1.8317102193832397
step: 91, loss: 1.285536527633667
step: 92, loss: 1.735977053642273
step: 93, loss: 9.7960205078125
step: 94, loss: 9.590989112854004
step: 95, loss: 9.139958381652832
step: 96, loss: 8.399389266967773
step: 97, loss: 7.600027084350586
step: 98, loss: 6.611666679382324
step: 99, loss: 6.208046913146973
step: 100, loss: 5.173367977142334
step: 101, loss: 4.322984218597412
step: 102, loss: 3.591176986694336
step: 103, loss: 2.806652307510376
step: 104, loss: 3.4043571949005127
step: 105, loss: 8.864307403564453
step: 106, loss: 8.512523651123047
step: 107, loss: 7.734834671020508
step: 108, loss: 7.761600971221924
step: 109, loss: 6.7916340827941895
step: 110, loss: 6.705571174621582
step: 111, loss: 5.521908760070801
step: 112, loss: 5.1336188316345215
step: 113, loss: 4.968385696411133
step: 114, loss: 4.201580047607422
step: 115, loss: 3.829331874847412
step: 116, loss: 4.398631572723389
step: 117, loss: 9.118717193603516
step: 118, loss: 8.640454292297363
step: 119, loss: 8.354057312011719
step: 120, loss: 8.468929290771484
step: 121, loss: 7.735539436340332
step: 122, loss: 7.644539833068848
step: 123, loss: 7.052806377410889
step: 124, loss: 6.791259765625
step: 125, loss: 6.580631732940674
step: 126, loss: 6.254650115966797
step: 127, loss: 6.010354518890381
step: 128, loss: 6.838200092315674
step: 129, loss: 11.156715393066406
step: 130, loss: 10.762813568115234
step: 131, loss: 10.67786979675293
step: 132, loss: 10.100110054016113
step: 133, loss: 9.83380126953125
step: 134, loss: 9.561834335327148
step: 135, loss: 9.356374740600586
step: 136, loss: 9.071183204650879
step: 137, loss: 8.940652847290039
step: 138, loss: 8.74654483795166
step: 139, loss: 8.452122688293457
step: 140, loss: 7.151215076446533
step: 141, loss: 4.231950759887695
step: 142, loss: 4.06899356842041
step: 143, loss: 3.9853556156158447
step: 144, loss: 3.7832164764404297
step: 145, loss: 3.7307136058807373
step: 146, loss: 3.4951767921447754
step: 147, loss: 3.3808066844940186
step: 148, loss: 3.2178430557250977
step: 149, loss: 3.009289503097534
step: 150, loss: 2.8369271755218506
step: 151, loss: 2.6734464168548584
step: 152, loss: 8.026894569396973
step: 153, loss: 12.058372497558594
step: 154, loss: 11.581417083740234
step: 155, loss: 11.61773681640625
step: 156, loss: 10.762090682983398
step: 157, loss: 10.503854751586914
step: 158, loss: 10.754670143127441
step: 159, loss: 10.436962127685547
step: 160, loss: 9.825800895690918
step: 161, loss: 9.606881141662598
step: 162, loss: 9.194146156311035
step: 163, loss: 9.065746307373047
step: 164, loss: 7.1887969970703125
step: 165, loss: 6.366128444671631
step: 166, loss: 6.128232479095459
step: 167, loss: 6.010615348815918
step: 168, loss: 5.893353462219238
step: 169, loss: 5.698566436767578
step: 170, loss: 5.479282379150391
step: 171, loss: 5.271546840667725
step: 172, loss: 5.076987266540527
step: 173, loss: 4.85135555267334
step: 174, loss: 4.680318832397461
step: 175, loss: 4.405458450317383
step: 176, loss: 8.601367950439453
step: 177, loss: 9.583625793457031
step: 178, loss: 9.501709938049316
step: 179, loss: 9.293741226196289
step: 180, loss: 9.069600105285645
step: 181, loss: 8.872357368469238
step: 182, loss: 8.654650688171387
step: 183, loss: 8.460302352905273
step: 184, loss: 8.278973579406738
step: 185, loss: 8.043464660644531
step: 186, loss: 7.820070266723633
step: 187, loss: 7.621276378631592
step: 188, loss: 7.641866683959961
step: 189, loss: 7.679172515869141
step: 190, loss: 7.517813682556152
step: 191, loss: 7.611003875732422
step: 192, loss: 7.195277214050293
step: 193, loss: 7.066830635070801
step: 194, loss: 6.85047721862793
step: 195, loss: 6.689709663391113
step: 196, loss: 6.410160541534424
step: 197, loss: 6.313629150390625
step: 198, loss: 6.003307342529297
step: 199, loss: 4.068540096282959
step: 200, loss: 3.623012065887451
step: 201, loss: 3.623569965362549
step: 202, loss: 3.425893545150757
step: 203, loss: 3.2689337730407715
step: 204, loss: 3.140094518661499
step: 205, loss: 2.918203353881836
step: 206, loss: 3.1871559619903564
step: 207, loss: 2.453397274017334
step: 208, loss: 2.296938896179199
step: 209, loss: 1.998715877532959
step: 210, loss: 5.666595935821533
step: 211, loss: 8.80797290802002
step: 212, loss: 8.889394760131836
step: 213, loss: 8.186156272888184
step: 214, loss: 8.508342742919922
step: 215, loss: 7.932468891143799
step: 216, loss: 7.776202201843262
step: 217, loss: 7.472919940948486
step: 218, loss: 7.672720909118652
step: 219, loss: 8.028358459472656
step: 220, loss: 10.051078796386719
step: 221, loss: 10.140344619750977
step: 222, loss: 9.844618797302246
step: 223, loss: 9.632801055908203
step: 224, loss: 9.422314643859863
step: 225, loss: 9.133151054382324
step: 226, loss: 3.026308536529541
step: 227, loss: 3.3439486026763916
step: 228, loss: 3.551511287689209
step: 229, loss: 2.692460298538208
step: 230, loss: 3.0843048095703125
step: 231, loss: 2.977912664413452
step: 232, loss: 3.2764530181884766
step: 233, loss: 3.2825303077697754
step: 234, loss: 3.0554754734039307
step: 235, loss: 3.281806230545044
step: 236, loss: 3.196732521057129
step: 237, loss: 3.89836049079895
step: 238, loss: 3.1390249729156494
step: 239, loss: 3.0462472438812256
step: 240, loss: 2.8066506385803223
step: 241, loss: 3.097933053970337
step: 242, loss: 2.8769726753234863
step: 243, loss: 2.995288610458374
step: 244, loss: 2.688687324523926
step: 245, loss: 2.5004308223724365
step: 246, loss: 2.5881507396698
step: 247, loss: 2.129365921020508
step: 248, loss: 2.6043505668640137
step: 249, loss: 2.7080044746398926
step: 250, loss: 2.4695589542388916
step: 251, loss: 2.091104030609131
step: 252, loss: 2.0347445011138916
step: 253, loss: 2.3804233074188232
step: 254, loss: 2.1368613243103027
step: 255, loss: 2.012657403945923
step: 256, loss: 2.1357004642486572
step: 257, loss: 1.909853458404541
step: 258, loss: 2.2329862117767334
step: 259, loss: 2.1390678882598877
step: 260, loss: 1.9705824851989746
step: 261, loss: 2.100977897644043
step: 262, loss: 2.3462636470794678
step: 263, loss: 1.6872562170028687
step: 264, loss: 1.9166696071624756
step: 265, loss: 1.9442543983459473
step: 266, loss: 1.8982107639312744
step: 267, loss: 1.7852402925491333
step: 268, loss: 1.3736305236816406
step: 269, loss: 1.7533962726593018
step: 270, loss: 1.955611228942871
step: 271, loss: 2.0827412605285645
step: 272, loss: 1.4885133504867554
step: 273, loss: 1.7428396940231323
step: 274, loss: 1.4606528282165527
step: 275, loss: 1.9118661880493164
step: 276, loss: 1.6778924465179443
step: 277, loss: 1.823123574256897
step: 278, loss: 1.4169297218322754
step: 279, loss: 1.6729626655578613
step: 280, loss: 1.5330604314804077
step: 281, loss: 1.4378794431686401
step: 282, loss: 1.658288598060608
step: 283, loss: 1.6667441129684448
step: 284, loss: 1.3630802631378174
step: 285, loss: 1.6292887926101685
step: 286, loss: 1.5223453044891357
step: 287, loss: 1.2362205982208252
step: 288, loss: 1.5029116868972778
step: 289, loss: 1.454483151435852
step: 290, loss: 1.708072543144226
step: 291, loss: 1.64035964012146
step: 292, loss: 1.4857432842254639
step: 293, loss: 1.6306031942367554
step: 294, loss: 1.1158157587051392
step: 295, loss: 1.2642359733581543
step: 296, loss: 1.404466986656189
step: 297, loss: 1.072616457939148
step: 298, loss: 1.358068585395813
step: 299, loss: 1.1973965167999268
step: 300, loss: 1.1150096654891968
step: 301, loss: 1.0591766834259033
step: 302, loss: 1.1285239458084106
step: 303, loss: 1.2590570449829102
step: 304, loss: 1.1048812866210938
step: 305, loss: 1.1161375045776367
step: 306, loss: 0.9569765329360962
step: 307, loss: 0.997982382774353
step: 308, loss: 1.1309829950332642
step: 309, loss: 1.0131431818008423
step: 310, loss: 0.8544257879257202
step: 311, loss: 0.8446868658065796
step: 312, loss: 0.657000720500946
step: 313, loss: 1.252064824104309
step: 314, loss: 1.0139564275741577
step: 315, loss: 1.2180824279785156
step: 316, loss: 1.1047606468200684
step: 317, loss: 1.196326732635498
step: 318, loss: 0.9922816753387451
step: 319, loss: 1.0448501110076904
step: 320, loss: 1.3651580810546875
step: 321, loss: 1.109574794769287
step: 322, loss: 1.0172420740127563
step: 323, loss: 1.27118718624115
step: 324, loss: 0.9152956604957581
step: 325, loss: 1.2324339151382446
step: 326, loss: 1.0291566848754883
step: 327, loss: 0.9288357496261597
step: 328, loss: 1.1173560619354248
step: 329, loss: 0.7676347494125366
step: 330, loss: 1.1888827085494995
step: 331, loss: 1.0650240182876587
step: 332, loss: 0.8896961212158203
step: 333, loss: 0.9847556352615356
step: 334, loss: 1.038692831993103
step: 335, loss: 0.9974883794784546
step: 336, loss: 0.7999090552330017
step: 337, loss: 0.9741377234458923
step: 338, loss: 0.823243260383606
step: 339, loss: 0.8003262281417847
step: 340, loss: 0.8015308380126953
step: 341, loss: 0.9587879180908203
step: 342, loss: 0.7975313663482666
step: 343, loss: 1.036463975906372
step: 344, loss: 0.7299457788467407
step: 345, loss: 0.7384803295135498
step: 346, loss: 0.7260681986808777
step: 347, loss: 0.9954319000244141
step: 348, loss: 0.9583065509796143
step: 349, loss: 0.817537248134613
step: 350, loss: 0.8595069050788879
step: 351, loss: 0.6520116925239563
step: 352, loss: 0.7253579497337341
step: 353, loss: 0.9300918579101562
step: 354, loss: 0.7230728268623352
step: 355, loss: 0.8910038471221924
step: 356, loss: 0.6008799076080322
step: 357, loss: 0.8586723208427429
step: 358, loss: 0.7724552154541016
step: 359, loss: 0.7143173217773438
step: 360, loss: 0.558258056640625
step: 361, loss: 0.815395712852478
step: 362, loss: 0.856192946434021
step: 363, loss: 0.823854923248291
step: 364, loss: 0.7806537747383118
step: 365, loss: 0.5751129388809204
step: 366, loss: 0.8971774578094482
step: 367, loss: 0.6669710278511047
step: 368, loss: 0.5144230127334595
step: 369, loss: 0.5146373510360718
step: 370, loss: 0.7306153178215027
step: 371, loss: 0.578356146812439
step: 372, loss: 0.4417290985584259
step: 373, loss: 0.7347384095191956
step: 374, loss: 0.6780927181243896
step: 375, loss: 0.6453631520271301
step: 376, loss: 0.7159375548362732
step: 377, loss: 0.6317387819290161
step: 378, loss: 0.4608857035636902
step: 379, loss: 0.7388911247253418
step: 380, loss: 0.6217236518859863
step: 381, loss: 0.5622612237930298
step: 382, loss: 0.5174559950828552
step: 383, loss: 0.5049745440483093
step: 384, loss: 0.44255006313323975
step: 385, loss: 0.7819815874099731
step: 386, loss: 0.48601508140563965
step: 387, loss: 0.5556275844573975
step: 388, loss: 0.8964715600013733
step: 389, loss: 0.37569689750671387
step: 390, loss: 0.6741329431533813
step: 391, loss: 0.4309466481208801
step: 392, loss: 0.4702071249485016
step: 393, loss: 0.2957140803337097
step: 394, loss: 0.39482706785202026
step: 395, loss: 0.5119439959526062
step: 396, loss: 0.6166390776634216
step: 397, loss: 0.522635281085968
step: 398, loss: 0.4387938678264618
step: 399, loss: 0.3012843430042267
step: 400, loss: 0.42158690094947815
step: 401, loss: 0.28520017862319946
step: 402, loss: 0.34126830101013184
step: 403, loss: 0.32744383811950684
step: 404, loss: 0.6029324531555176
step: 405, loss: 0.4952532947063446
step: 406, loss: 0.5040861368179321
step: 407, loss: 0.3536698818206787
step: 408, loss: 0.44807565212249756
step: 409, loss: 0.5521633625030518
step: 410, loss: 0.3897353410720825
step: 411, loss: 0.31668809056282043
step: 412, loss: 0.49413520097732544
step: 413, loss: 0.4529537558555603
step: 414, loss: 0.586765468120575
step: 415, loss: 0.3961797058582306
step: 416, loss: 0.6464029550552368
step: 417, loss: 0.5237024426460266
step: 418, loss: 0.4047144949436188
step: 419, loss: 0.28089630603790283
step: 420, loss: 0.5191171169281006
step: 421, loss: 0.40322670340538025
step: 422, loss: 0.24105113744735718
step: 423, loss: 0.25472167134284973
step: 424, loss: 0.44613420963287354
step: 425, loss: 0.2820867598056793
step: 426, loss: 0.5381690859794617
step: 427, loss: 0.3511967957019806
step: 428, loss: 0.6362237334251404
step: 429, loss: 0.34002062678337097
step: 430, loss: 0.41426292061805725
step: 431, loss: 0.6112828254699707
step: 432, loss: 0.6036781668663025
step: 433, loss: 0.31209075450897217
step: 434, loss: 0.40992528200149536
step: 435, loss: 0.4459865093231201
step: 436, loss: 0.13016144931316376
step: 437, loss: 0.4350535571575165
step: 438, loss: 0.5017354488372803
step: 439, loss: 0.5594147443771362
step: 440, loss: 0.34567588567733765
step: 441, loss: 0.35602015256881714
step: 442, loss: 0.6699368357658386
step: 443, loss: 0.3792867958545685
step: 444, loss: 0.2489001601934433
step: 445, loss: 0.4031858444213867
step: 446, loss: 0.2688523530960083
step: 447, loss: 0.5560345649719238
step: 448, loss: 0.15586787462234497
step: 449, loss: 0.35413989424705505
step: 450, loss: 0.4382588267326355
step: 451, loss: 0.5050981640815735
step: 452, loss: 0.12684234976768494
step: 453, loss: 0.0836876854300499
step: 454, loss: 0.06085870414972305
step: 455, loss: 0.16128133237361908
step: 456, loss: 0.1029956117272377
step: 457, loss: 0.2749685049057007
step: 458, loss: 0.12736083567142487
step: 459, loss: 0.08514241874217987
step: 460, loss: 0.09283841401338577
step: 461, loss: 0.18157859146595
step: 462, loss: 0.28870707750320435
step: 463, loss: 0.16024677455425262
step: 464, loss: 0.20068450272083282
step: 465, loss: 0.09298621863126755
step: 466, loss: 0.13325808942317963
step: 467, loss: 0.14984376728534698
step: 468, loss: 0.23049339652061462
step: 469, loss: 0.15107303857803345
step: 470, loss: 0.15152062475681305
step: 471, loss: 0.11081628501415253
step: 472, loss: 0.12379255890846252
step: 473, loss: 0.0995481088757515
step: 474, loss: 0.19105973839759827
step: 475, loss: 0.13056974112987518
step: 476, loss: 0.09999152272939682
step: 477, loss: 0.1288205087184906
step: 478, loss: 0.17975008487701416
step: 479, loss: 0.13133619725704193
step: 480, loss: 0.18368123471736908
step: 481, loss: 0.1688385158777237
step: 482, loss: 0.056024909019470215
step: 483, loss: 0.07061511278152466
step: 484, loss: 0.13876889646053314
step: 485, loss: 0.10872496664524078
step: 486, loss: 0.18086302280426025
step: 487, loss: 0.3306259512901306
step: 488, loss: 0.06136525049805641
step: 489, loss: 0.14405855536460876
step: 490, loss: 0.24772942066192627
step: 491, loss: 0.07553865760564804
step: 492, loss: 0.06691260635852814
step: 493, loss: 0.09050280600786209
step: 494, loss: 0.10551101714372635
step: 495, loss: 0.16142655909061432
step: 496, loss: 0.1697515845298767
step: 497, loss: 0.43417495489120483
step: 498, loss: 0.35183238983154297
step: 499, loss: 0.18125241994857788
step: 500, loss: 0.07135406136512756
step: 501, loss: 0.10666909068822861
step: 502, loss: 0.3023359775543213
step: 503, loss: 0.16764315962791443
step: 504, loss: 0.19117945432662964
step: 505, loss: 0.06649798154830933
step: 506, loss: 0.06079685688018799
step: 507, loss: 0.05737404525279999
step: 508, loss: 0.11923836916685104
step: 509, loss: 0.16590146720409393
step: 510, loss: 0.23646709322929382
step: 511, loss: 0.13980062305927277
step: 512, loss: 0.06507343053817749
step: 513, loss: 0.24433134496212006
step: 514, loss: 0.11918208748102188
step: 515, loss: 0.0804060623049736
step: 516, loss: 0.15106768906116486
step: 517, loss: 0.17779259383678436
step: 518, loss: 0.06365741044282913
step: 519, loss: 0.11553137004375458
step: 520, loss: 0.09836918115615845
step: 521, loss: 0.14862479269504547
step: 522, loss: 0.10641085356473923
step: 523, loss: 0.09105998277664185
step: 524, loss: 0.03721414506435394
step: 525, loss: 0.2134617269039154
step: 526, loss: 0.14502006769180298
step: 527, loss: 0.15147435665130615
step: 528, loss: 0.12111964821815491
step: 529, loss: 0.13286873698234558
step: 530, loss: 0.20034031569957733
step: 531, loss: 0.30767837166786194
step: 532, loss: 0.12696895003318787
step: 533, loss: 0.1354881078004837
step: 534, loss: 0.2655225992202759
step: 535, loss: 0.19575181603431702
step: 536, loss: 0.28148043155670166
step: 537, loss: 0.259896844625473
step: 538, loss: 0.0619436539709568
step: 539, loss: 0.1116187572479248
step: 540, loss: 0.1124761775135994
step: 541, loss: 0.04201687499880791
step: 542, loss: 0.031185926869511604
step: 543, loss: 0.15931716561317444
step: 544, loss: 0.09467120468616486
step: 545, loss: 0.12246759235858917
step: 546, loss: 0.15627562999725342
step: 547, loss: 0.1267309933900833
step: 548, loss: 0.20997600257396698
step: 549, loss: 0.13607163727283478
step: 550, loss: 0.07136177271604538
step: 551, loss: 0.24102632701396942
step: 552, loss: 0.19494439661502838
step: 553, loss: 0.13944263756275177
step: 554, loss: 0.20986761152744293
step: 555, loss: 0.14689412713050842
step: 556, loss: 0.0599512904882431
step: 557, loss: 0.03713488578796387
step: 558, loss: 0.08975473046302795
step: 559, loss: 0.1114155575633049
step: 560, loss: 0.10945797711610794
step: 561, loss: 0.15309913456439972
step: 562, loss: 0.027024637907743454
step: 563, loss: 0.11431432515382767
step: 564, loss: 0.12408219277858734
step: 565, loss: 0.16140085458755493
step: 566, loss: 0.23712894320487976
step: 567, loss: 0.16426029801368713
step: 568, loss: 0.1272452026605606
step: 569, loss: 0.08685480058193207
step: 570, loss: 0.05716688558459282
step: 571, loss: 0.07434530556201935
step: 572, loss: 0.014311629347503185
step: 573, loss: 0.12894976139068604
step: 574, loss: 0.2025776505470276
step: 575, loss: 0.12406598031520844
step: 576, loss: 0.1612834930419922
step: 577, loss: 0.09918925166130066
step: 578, loss: 0.14856380224227905
step: 579, loss: 0.09524102509021759
step: 580, loss: 0.17084170877933502
step: 581, loss: 0.18356990814208984
step: 582, loss: 0.1542695313692093
step: 583, loss: 0.1514575481414795
step: 584, loss: 0.135409876704216
step: 585, loss: 0.14112314581871033
step: 586, loss: 0.12327057123184204
step: 587, loss: 0.12330994009971619
step: 588, loss: 0.08719883114099503
step: 589, loss: 0.18634489178657532
step: 590, loss: 0.08634799718856812
step: 591, loss: 0.12779612839221954
step: 592, loss: 0.034305691719055176
step: 593, loss: 0.05085026100277901
step: 594, loss: 0.09235494583845139
step: 595, loss: 0.1454990804195404
step: 596, loss: 0.05881960317492485
step: 597, loss: 0.0633370578289032
step: 598, loss: 0.14039720594882965
step: 599, loss: 0.09335657954216003
step: 600, loss: 0.0834675207734108
step: 601, loss: 0.10119283944368362
step: 602, loss: 0.06832034140825272
step: 603, loss: 0.10833100974559784
step: 604, loss: 0.09022662788629532
step: 605, loss: 0.061457280069589615
step: 606, loss: 0.1460798680782318
step: 607, loss: 0.11079223453998566
step: 608, loss: 0.0304380115121603
step: 609, loss: 0.032645486295223236
step: 610, loss: 0.11431490629911423
step: 611, loss: 0.16127942502498627
step: 612, loss: 0.1399315446615219
step: 613, loss: 0.09052655100822449
step: 614, loss: 0.24003912508487701
step: 615, loss: 0.09746716171503067
step: 616, loss: 0.3175092935562134
step: 617, loss: 0.30223962664604187
step: 618, loss: 0.04657566547393799
step: 619, loss: 0.02099030651152134
step: 620, loss: 0.05280248820781708
step: 621, loss: 0.2917003333568573
step: 622, loss: 0.10501515120267868
step: 623, loss: 0.20919111371040344
step: 624, loss: 0.3049084544181824
step: 625, loss: 0.12440347671508789
step: 626, loss: 0.1525244265794754
step: 627, loss: 0.1775692105293274
step: 628, loss: 0.1044137179851532
step: 629, loss: 0.08949220925569534
step: 630, loss: 0.0588267520070076
step: 631, loss: 0.12042791396379471
step: 632, loss: 0.05467529222369194
step: 633, loss: 0.2925494611263275
step: 634, loss: 0.09771104156970978
step: 635, loss: 0.15975217521190643
step: 636, loss: 0.12005934119224548
step: 637, loss: 0.08386202901601791
step: 638, loss: 0.2325487732887268
step: 639, loss: 0.04053327068686485
step: 640, loss: 0.23783749341964722
step: 641, loss: 0.030382174998521805
step: 642, loss: 0.16474750638008118
step: 643, loss: 0.13161569833755493
step: 644, loss: 0.06332694739103317
step: 645, loss: 0.20566748082637787
step: 646, loss: 0.09623033553361893
step: 647, loss: 0.07902209460735321
step: 648, loss: 0.017953181639313698
step: 649, loss: 0.07772644609212875
step: 650, loss: 0.12830227613449097
step: 651, loss: 0.13145901262760162
step: 652, loss: 0.1389838457107544
step: 653, loss: 0.16347993910312653
step: 654, loss: 0.024541614577174187
step: 655, loss: 0.06184825301170349
step: 656, loss: 0.14298702776432037
step: 657, loss: 0.09553974866867065
step: 658, loss: 0.14946255087852478
step: 659, loss: 0.125627338886261
step: 660, loss: 0.3035086393356323
step: 661, loss: 0.3316574990749359
step: 662, loss: 0.17254933714866638
step: 663, loss: 0.16442355513572693
step: 664, loss: 0.1352986842393875
step: 665, loss: 0.2171126753091812
step: 666, loss: 0.0984044149518013
step: 667, loss: 0.19804799556732178
step: 668, loss: 0.16810579597949982
step: 669, loss: 0.15848512947559357
step: 670, loss: 0.05660746246576309
step: 671, loss: 0.16983631253242493
step: 672, loss: 0.12339809536933899
step: 673, loss: 0.13924740254878998
step: 674, loss: 0.11505117267370224
step: 675, loss: 0.25102904438972473
step: 676, loss: 0.07706725597381592
step: 677, loss: 0.08667846024036407
step: 678, loss: 0.03270113095641136
step: 679, loss: 0.03691716119647026
step: 680, loss: 0.02300916239619255
step: 681, loss: 0.013587355613708496
step: 682, loss: 0.017376430332660675
step: 683, loss: 0.008350992575287819
step: 684, loss: 0.009005404077470303
step: 685, loss: 0.01745450124144554
step: 686, loss: 0.01651851087808609
step: 687, loss: 0.011651533655822277
step: 688, loss: 0.009024124592542648
step: 689, loss: 0.010508524253964424
step: 690, loss: 0.010653562843799591
step: 691, loss: 0.01497852336615324
step: 692, loss: 0.006950256880372763
step: 693, loss: 0.13833020627498627
step: 694, loss: 0.012907295487821102
step: 695, loss: 0.060888390988111496
step: 696, loss: 0.00990289170295
step: 697, loss: 0.022600295022130013
step: 698, loss: 0.003928785212337971
step: 699, loss: 0.0082055339589715
step: 700, loss: 0.004439536016434431
step: 701, loss: 0.01673273742198944
step: 702, loss: 0.010173125192523003
step: 703, loss: 0.00732598127797246
step: 704, loss: 0.019954942166805267
step: 705, loss: 0.020916223526000977
step: 706, loss: 0.015687081962823868
step: 707, loss: 0.07913286983966827
step: 708, loss: 0.019555285573005676
step: 709, loss: 0.01046409085392952
step: 710, loss: 0.025303244590759277
step: 711, loss: 0.008801999501883984
step: 712, loss: 0.011487429030239582
step: 713, loss: 0.007191705517470837
step: 714, loss: 0.012790757231414318
step: 715, loss: 0.0056282221339643
step: 716, loss: 0.01626436971127987
step: 717, loss: 0.043830547481775284
step: 718, loss: 0.024306796491146088
step: 719, loss: 0.014522147364914417
step: 720, loss: 0.019385406747460365
step: 721, loss: 0.023808762431144714
step: 722, loss: 0.0225577000528574
step: 723, loss: 0.016110410913825035
step: 724, loss: 0.05633534863591194
step: 725, loss: 0.042960748076438904
step: 726, loss: 0.019112367182970047
step: 727, loss: 0.08993244916200638
step: 728, loss: 0.007684492040425539
step: 729, loss: 0.08054646849632263
step: 730, loss: 0.011494443751871586
step: 731, loss: 0.0795232281088829
step: 732, loss: 0.006630113814026117
step: 733, loss: 0.0057762484066188335
step: 734, loss: 0.020453087985515594
step: 735, loss: 0.02437642030417919
step: 736, loss: 0.027712814509868622
step: 737, loss: 0.05648573860526085
step: 738, loss: 0.011919784359633923
step: 739, loss: 0.008686179295182228
step: 740, loss: 0.013336734846234322
step: 741, loss: 0.007325949613004923
step: 742, loss: 0.07776585221290588
step: 743, loss: 0.015174328349530697
step: 744, loss: 0.012493375688791275
step: 745, loss: 0.01351166982203722
step: 746, loss: 0.015109593980014324
step: 747, loss: 0.08785688132047653
step: 748, loss: 0.009086898528039455
step: 749, loss: 0.005259601399302483
step: 750, loss: 0.018745984882116318
step: 751, loss: 0.012636793777346611
step: 752, loss: 0.008089477196335793
step: 753, loss: 0.024986423552036285
step: 754, loss: 0.030729033052921295
step: 755, loss: 0.008709602989256382
step: 756, loss: 0.04175487160682678
step: 757, loss: 0.07966728508472443
step: 758, loss: 0.061289358884096146
step: 759, loss: 0.004589408170431852
step: 760, loss: 0.007861404679715633
step: 761, loss: 0.013778355903923512
step: 762, loss: 0.0202792901545763
step: 763, loss: 0.015668515115976334
step: 764, loss: 0.01064050942659378
step: 765, loss: 0.058120179921388626
step: 766, loss: 0.009834435768425465
step: 767, loss: 0.009739979170262814
step: 768, loss: 0.011264904402196407
step: 769, loss: 0.035745106637477875
step: 770, loss: 0.01676500029861927
step: 771, loss: 0.01767871528863907
step: 772, loss: 0.006526136305183172
step: 773, loss: 0.04239201545715332
step: 774, loss: 0.012582370080053806
step: 775, loss: 0.01637006551027298
step: 776, loss: 0.0321878083050251
step: 777, loss: 0.040341462939977646
step: 778, loss: 0.017776181921362877
step: 779, loss: 0.017545495182275772
step: 780, loss: 0.012793922796845436
step: 781, loss: 0.01811274141073227
step: 782, loss: 0.011499748565256596
step: 783, loss: 0.10518337041139603
step: 784, loss: 0.012073146179318428
step: 785, loss: 0.021429292857646942
step: 786, loss: 0.004019896034151316
step: 787, loss: 0.006590347271412611
step: 788, loss: 0.027099044993519783
step: 789, loss: 0.025654755532741547
step: 790, loss: 0.01888214237987995
step: 791, loss: 0.02152152732014656
step: 792, loss: 0.012566489167511463
step: 793, loss: 0.006978292018175125
step: 794, loss: 0.04342414066195488
step: 795, loss: 0.0035180833656340837
step: 796, loss: 0.006864800583571196
step: 797, loss: 0.06140221282839775
step: 798, loss: 0.01994960382580757
step: 799, loss: 0.06851722300052643
step: 800, loss: 0.015776943415403366
step: 801, loss: 0.016864240169525146
step: 802, loss: 0.06404367834329605
step: 803, loss: 0.011597586795687675
step: 804, loss: 0.01098591834306717
step: 805, loss: 0.023151833564043045
step: 806, loss: 0.013950347900390625
step: 807, loss: 0.010022743605077267
step: 808, loss: 0.010286631993949413
step: 809, loss: 0.018117045983672142
step: 810, loss: 0.015503967180848122
step: 811, loss: 0.00922025740146637
step: 812, loss: 0.026536734774708748
step: 813, loss: 0.0633307546377182
step: 814, loss: 0.012967641465365887
step: 815, loss: 0.008814995177090168
step: 816, loss: 0.010581579990684986
step: 817, loss: 0.01553467195481062
step: 818, loss: 0.008004283532500267
step: 819, loss: 0.028392845764756203
step: 820, loss: 0.0064064073376357555
step: 821, loss: 0.007050784304738045
step: 822, loss: 0.003420077031478286
step: 823, loss: 0.017288394272327423
step: 824, loss: 0.010458596050739288
step: 825, loss: 0.011470550671219826
step: 826, loss: 0.07456687092781067
step: 827, loss: 0.023761700838804245
step: 828, loss: 0.019731253385543823
step: 829, loss: 0.01237609051167965
step: 830, loss: 0.004021538887172937
step: 831, loss: 0.00810007844120264
step: 832, loss: 0.016252301633358
step: 833, loss: 0.006334954407066107
step: 834, loss: 0.010501847602427006
step: 835, loss: 0.01174778863787651
step: 836, loss: 0.03428323566913605
step: 837, loss: 0.024619318544864655
step: 838, loss: 0.004812882747501135
step: 839, loss: 0.009792232885956764
step: 840, loss: 0.006365720182657242
step: 841, loss: 0.004853383172303438
step: 842, loss: 0.014153244905173779
step: 843, loss: 0.019181884825229645
step: 844, loss: 0.03058522939682007
step: 845, loss: 0.04116515815258026
step: 846, loss: 0.1194959431886673
step: 847, loss: 0.10917608439922333
step: 848, loss: 0.08831758797168732
step: 849, loss: 0.020473001524806023
step: 850, loss: 0.00894803088158369
step: 851, loss: 0.03999440371990204
step: 852, loss: 0.00894900318235159
step: 853, loss: 0.027306968346238136
step: 854, loss: 0.09989666938781738
step: 855, loss: 0.023042313754558563
step: 856, loss: 0.12287454307079315
step: 857, loss: 0.029633842408657074
step: 858, loss: 0.004712509457021952
step: 859, loss: 0.018887696787714958
step: 860, loss: 0.00787336379289627
step: 861, loss: 0.017029356211423874
step: 862, loss: 0.013123959302902222
step: 863, loss: 0.005433989223092794
step: 864, loss: 0.01130103599280119
step: 865, loss: 0.05399028956890106
step: 866, loss: 0.012731063179671764
step: 867, loss: 0.018258720636367798
step: 868, loss: 0.011132882907986641
step: 869, loss: 0.00946800783276558
step: 870, loss: 0.0067344591952860355
step: 871, loss: 0.007779859472066164
step: 872, loss: 0.008950099349021912
step: 873, loss: 0.008396192453801632
step: 874, loss: 0.005842939484864473
step: 875, loss: 0.026201702654361725
step: 876, loss: 0.005917759146541357
step: 877, loss: 0.017178798094391823
step: 878, loss: 0.012826385907828808
step: 879, loss: 0.006089113187044859
step: 880, loss: 0.016304368153214455
step: 881, loss: 0.004498105496168137
step: 882, loss: 0.01827685721218586
step: 883, loss: 0.0032083848491311073
step: 884, loss: 0.029184110462665558
step: 885, loss: 0.00650065066292882
step: 886, loss: 0.0033640468027442694
step: 887, loss: 0.033964768052101135
step: 888, loss: 0.006528873462229967
step: 889, loss: 0.0197604987770319
step: 890, loss: 0.012845328077673912
step: 891, loss: 0.005460683722048998
step: 892, loss: 0.0055142720229923725
step: 893, loss: 0.011626677587628365
step: 894, loss: 0.01216711476445198
step: 895, loss: 0.0032720952294766903
step: 896, loss: 0.022680895403027534
step: 897, loss: 0.01578492671251297
step: 898, loss: 0.005338383372873068
step: 899, loss: 0.04731670394539833
step: 900, loss: 0.008166075684130192
step: 901, loss: 0.034350331872701645
step: 902, loss: 0.06099850311875343
step: 903, loss: 0.08286885172128677
step: 904, loss: 0.015904700383543968
step: 905, loss: 0.0041664582677185535
step: 906, loss: 0.0038776809815317392
step: 907, loss: 0.0035584066063165665
step: 908, loss: 0.002832654630765319
step: 909, loss: 0.0033337827771902084
step: 910, loss: 0.0036887836176902056
step: 911, loss: 0.00503501994535327
step: 912, loss: 0.004331890493631363
step: 913, loss: 0.08502237498760223
step: 914, loss: 0.0033911101054400206
step: 915, loss: 0.005340678617358208
step: 916, loss: 0.006461755838245153
step: 917, loss: 0.0030578761361539364
step: 918, loss: 0.0023942161351442337
step: 919, loss: 0.0045853168703615665
step: 920, loss: 0.0038438409101217985
step: 921, loss: 0.003347305580973625
step: 922, loss: 0.00983639620244503
step: 923, loss: 0.00416147243231535
step: 924, loss: 0.004989126697182655
step: 925, loss: 0.020375480875372887
step: 926, loss: 0.005759756546467543
step: 927, loss: 0.030706629157066345
step: 928, loss: 0.004992096219211817
step: 929, loss: 0.006659262347966433
step: 930, loss: 0.003583865240216255
step: 931, loss: 0.002396149095147848
step: 932, loss: 0.004774915520101786
step: 933, loss: 0.004139321856200695
step: 934, loss: 0.005044159479439259
step: 935, loss: 0.013215080834925175
step: 936, loss: 0.0028440963942557573
step: 937, loss: 0.003612865461036563
step: 938, loss: 0.004158345982432365
step: 939, loss: 0.0050302534364163876
step: 940, loss: 0.004440395161509514
step: 941, loss: 0.004867470823228359
step: 942, loss: 0.014576294459402561
step: 943, loss: 0.009336845017969608
step: 944, loss: 0.003493791911751032
step: 945, loss: 0.004827271215617657
step: 946, loss: 0.003519889200106263
step: 947, loss: 0.004648377187550068
step: 948, loss: 0.0031111747957766056
step: 949, loss: 0.0053111170418560505
step: 950, loss: 0.003141966415569186
step: 951, loss: 0.011307913810014725
step: 952, loss: 0.0015270599396899343
step: 953, loss: 0.05484466999769211
step: 954, loss: 0.010384226217865944
step: 955, loss: 0.003032664768397808
step: 956, loss: 0.034683771431446075
step: 957, loss: 0.0028943740762770176
step: 958, loss: 0.005490775685757399
step: 959, loss: 0.003024376928806305
step: 960, loss: 0.004366995766758919
step: 961, loss: 0.004791629035025835
step: 962, loss: 0.00580344395712018
step: 963, loss: 0.005168896168470383
step: 964, loss: 0.00185017054900527
step: 965, loss: 0.002684494014829397
step: 966, loss: 0.027865735813975334
step: 967, loss: 0.005036019254475832
step: 968, loss: 0.005612090229988098
step: 969, loss: 0.005203723441809416
step: 970, loss: 0.0028911905828863382
step: 971, loss: 0.0068169510923326015
step: 972, loss: 0.028709562495350838
step: 973, loss: 0.002577511128038168
step: 974, loss: 0.0036884162109345198
step: 975, loss: 0.0027567630168050528
step: 976, loss: 0.0023442504461854696
step: 977, loss: 0.005120621528476477
step: 978, loss: 0.06425971537828445
step: 979, loss: 0.004830008838325739
step: 980, loss: 0.019140373915433884
step: 981, loss: 0.00905680377036333
step: 982, loss: 0.00490164291113615
step: 983, loss: 0.002220009220764041
step: 984, loss: 0.0045149195939302444
step: 985, loss: 0.003568189451470971
step: 986, loss: 0.14812225103378296
step: 987, loss: 0.04321625828742981
step: 988, loss: 0.004639401566237211
step: 989, loss: 0.004218182060867548
step: 990, loss: 0.004863584414124489
step: 991, loss: 0.004789938684552908
step: 992, loss: 0.0027130302041769028
step: 993, loss: 0.0031538784969598055
step: 994, loss: 0.002628946676850319
step: 995, loss: 0.0037306142039597034
step: 996, loss: 0.00763901649042964
step: 997, loss: 0.005706203170120716
step: 998, loss: 0.004471292719244957
step: 999, loss: 0.0040874541737139225
step: 1000, loss: 0.011307964101433754
Epoch: 4
Accuracy on test data: 0.7992565055762082